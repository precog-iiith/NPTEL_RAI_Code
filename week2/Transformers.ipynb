{"cells":[{"cell_type":"markdown","id":"bfc1b0ba-2469-4ec6-821a-372139d83fac","metadata":{"id":"bfc1b0ba-2469-4ec6-821a-372139d83fac"},"source":["<h1 style=\"font-size:60px;\"><center>Transformer</center></h1>"]},{"cell_type":"markdown","id":"a6eda700-90b7-41a6-85d3-70ce023703cc","metadata":{"id":"a6eda700-90b7-41a6-85d3-70ce023703cc"},"source":["<div style=\"text-align:center\"><img src=\"https://i.kym-cdn.com/entries/icons/original/000/036/585/Attention_is_all_you_need.jpg\" /></div>"]},{"cell_type":"markdown","id":"f1b20477-0f01-4aba-93b5-ddeed3f68d1d","metadata":{"id":"f1b20477-0f01-4aba-93b5-ddeed3f68d1d"},"source":["# Prerequisites\n","\n","+ Python\n","+ Working knowledge of Pytorch\n","+ Neural Networks\n","+ Seq2Seq\n","+ Attention"]},{"cell_type":"markdown","id":"1d159f0f-1d3a-4995-86c1-dee8c61103d7","metadata":{"id":"1d159f0f-1d3a-4995-86c1-dee8c61103d7"},"source":["# Where did it come from?"]},{"cell_type":"markdown","id":"90a047d8-47b4-4673-a6de-4745aae5a473","metadata":{"id":"90a047d8-47b4-4673-a6de-4745aae5a473"},"source":["&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;RNN &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; BERT\n","\n","&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;LSTM & GRU  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ⬅ **2017 - [Attention Is All You Need](https://arxiv.org/pdf/1706.03762)** ➡️ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; GPT\n","\n","&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;Attention &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Vision Transformer"]},{"cell_type":"code","execution_count":null,"id":"90f26276-1a93-4dad-8954-f7785d467fb6","metadata":{"id":"90f26276-1a93-4dad-8954-f7785d467fb6"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import math\n","import numpy as np"]},{"cell_type":"markdown","id":"f15c8ef1-904f-442d-848d-af4cdd2026e3","metadata":{"id":"f15c8ef1-904f-442d-848d-af4cdd2026e3"},"source":["# The Architutre"]},{"cell_type":"markdown","id":"660775d3-38ae-47b4-a3a0-1c8fac48e7d4","metadata":{"id":"660775d3-38ae-47b4-a3a0-1c8fac48e7d4"},"source":["<div style=\"text-align:center\">\n","  <img src=\"https://miro.medium.com/v2/resize:fit:856/1*ZCFSvkKtppgew3cc7BIaug.png\" width=\"500px\"/>\n","</div>\n"]},{"cell_type":"markdown","id":"fb6bba43-d9f8-4665-838f-952e8c594bcd","metadata":{"id":"fb6bba43-d9f8-4665-838f-952e8c594bcd"},"source":["# Multi-Head Attention"]},{"cell_type":"markdown","id":"3b1c617f-6717-4fe9-b9cd-10778a7f1301","metadata":{"id":"3b1c617f-6717-4fe9-b9cd-10778a7f1301"},"source":["## Self Attention"]},{"cell_type":"markdown","id":"f1644eb5-9f2e-4a3a-ab7d-58c74e78bbef","metadata":{"id":"f1644eb5-9f2e-4a3a-ab7d-58c74e78bbef"},"source":["<div style=\"text-align:center\">\n","  <img src=\"https://i.imgur.com/thdHvQx.png\" width=\"400px\"/>\n","</div>"]},{"cell_type":"code","execution_count":null,"id":"1044bd85-1e67-401d-9c5c-3d33a7724c4f","metadata":{"id":"1044bd85-1e67-401d-9c5c-3d33a7724c4f"},"outputs":[],"source":["class DummyAttention:\n","    def __init__(self):\n","        self.data = np.random.normal(size=(50,50))\n","\n","        self.weight_key = np.random.normal(size=(50,50))\n","        self.weight_query = np.random.normal(size=(50,50))\n","        self.weight_value = np.random.normal(size=(50,50))\n","\n","    def key(self):\n","        return self.weight_key @ self.data\n","\n","    def query(self):\n","        return self.weight_query @ self.data\n","\n","    def value(self):\n","        return self.weight_value @ self.data\n","\n","    def forward(self):\n","        k = self.key()\n","        q = self.query()\n","        v = self.value()\n","\n","        scores = k @ q.T\n","        scores = scores / k.shape[0]**0.5\n","        scores = np.exp(scores) / np.sum(scores)\n","\n","\n","        attn = scores @ v\n","        return attn\n"]},{"cell_type":"code","execution_count":null,"id":"0866376f-f9ae-4ff3-aafb-30ec77af7ba1","metadata":{"id":"0866376f-f9ae-4ff3-aafb-30ec77af7ba1","outputId":"447fcc99-c899-4d18-c012-5a8d523168a7"},"outputs":[{"name":"stdout","output_type":"stream","text":["(50, 50)\n"]},{"data":{"text/plain":["array([[-8.30195779e+65, -8.89317968e+66,  7.07996198e+66, ...,\n","        -2.99217010e+66, -4.35827153e+66, -2.28896438e+65],\n","       [ 4.63385359e+61, -3.15115673e+61, -1.77858341e+62, ...,\n","         3.21194788e+61,  9.01899169e+60, -1.19295385e+62],\n","       [ 1.58280149e+99,  8.45229339e+98,  1.02801068e+99, ...,\n","        -6.74607218e+98, -5.04194860e+98,  4.05223682e+98],\n","       ...,\n","       [-1.66695826e+77, -4.26879902e+76, -3.32295849e+77, ...,\n","        -6.45407153e+76, -5.07158546e+77,  1.05065407e+77],\n","       [ 2.54000235e+66, -7.30570093e+66, -5.51913866e+66, ...,\n","         1.32398738e+66, -1.83213148e+66, -5.29677704e+66],\n","       [ 7.89281451e+57, -9.33433065e+57, -1.23080627e+58, ...,\n","        -5.40586441e+57, -3.24482361e+58,  7.45437755e+57]])"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["a = DummyAttention()\n","att = a.forward()\n","print(att.shape)\n","att"]},{"cell_type":"markdown","id":"c293eb63-e4a8-443e-9f14-82aaef21d748","metadata":{"id":"c293eb63-e4a8-443e-9f14-82aaef21d748"},"source":["## Multi-Head"]},{"cell_type":"markdown","id":"fb77bbe0-d470-4079-b4f7-51d33d9fe902","metadata":{"id":"fb77bbe0-d470-4079-b4f7-51d33d9fe902"},"source":["<div style=\"text-align:center\">\n","  <img src=\"https://miro.medium.com/v2/resize:fit:1010/0*0KPEV8QidHkteKeY.png\" width=\"500px\"/>\n","</div>"]},{"cell_type":"code","execution_count":null,"id":"abf1eff9-1357-449b-b71c-fd04bf58f573","metadata":{"id":"abf1eff9-1357-449b-b71c-fd04bf58f573"},"outputs":[],"source":["class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_model, num_heads):\n","        super(MultiHeadAttention, self).__init__()\n","\n","        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n","\n","        self.d_model = d_model\n","        self.num_heads = num_heads\n","        self.d_k = d_model // num_heads\n","\n","        self.query = nn.Linear(d_model, d_model)\n","        self.key = nn.Linear(d_model, d_model)\n","        self.value = nn.Linear(d_model, d_model)\n","\n","        self.out = nn.Linear(d_model, d_model)\n","\n","    def forward(self, q, k, v, mask=None):\n","        batch_size = q.size(0)\n","\n","        # Perform linear projections\n","        Q = self.query(q)  # (batch_size, seq_length, d_model)\n","        K = self.key(k)    # (batch_size, seq_length, d_model)\n","        V = self.value(v)  # (batch_size, seq_length, d_model)\n","\n","        # Split the projections into multiple heads and reshape\n","        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)  # (batch_size, num_heads, seq_length, d_k)\n","        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)  # (batch_size, num_heads, seq_length, d_k)\n","        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)  # (batch_size, num_heads, seq_length, d_k)\n","\n","        # Calculate attention scores\n","        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)  # (batch_size, num_heads, seq_length, seq_length)\n","\n","        if mask is not None:\n","            mask = torch.broadcast_to(mask.unsqueeze(1), (batch_size, self.num_heads, mask.shape[-1],mask.shape[-1]))\n","            scores = scores.masked_fill(mask == 0, -1e20)\n","\n","        attention_weights = F.softmax(scores, dim=-1)  # (batch_size, num_heads, seq_length, seq_length)\n","\n","        # Apply attention weights to the values\n","        attention_output = torch.matmul(attention_weights, V)  # (batch_size, num_heads, seq_length, d_k)\n","\n","        # Concatenate the heads and reshape\n","        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)  # (batch_size, seq_length, d_model)\n","\n","        # Final linear layer\n","        output = self.out(attention_output)  # (batch_size, seq_length, d_model)\n","\n","        return output"]},{"cell_type":"markdown","id":"08cb3fbf-bd93-41ee-8c5a-72c8b397a35b","metadata":{"id":"08cb3fbf-bd93-41ee-8c5a-72c8b397a35b"},"source":["# Encoder"]},{"cell_type":"markdown","id":"532d912a-5726-434c-a18e-aa3207b182e9","metadata":{"id":"532d912a-5726-434c-a18e-aa3207b182e9"},"source":["## Feed Forward"]},{"cell_type":"code","execution_count":null,"id":"54acabb8-5105-42ca-9ee1-e55488d35803","metadata":{"id":"54acabb8-5105-42ca-9ee1-e55488d35803"},"outputs":[],"source":["class FeedForward(nn.Module):\n","    def __init__(self, d_model, d_ff):\n","        super(FeedForward, self).__init__()\n","        self.linear1 = nn.Linear(d_model, d_ff)\n","        self.linear2 = nn.Linear(d_ff, d_model)\n","\n","    def forward(self, x):\n","        x = F.relu(self.linear1(x))\n","        x = self.linear2(x)\n","        return x"]},{"cell_type":"markdown","id":"744e7b43-c4e9-402e-bb77-1a16777af83a","metadata":{"id":"744e7b43-c4e9-402e-bb77-1a16777af83a"},"source":["## Encoder layer"]},{"cell_type":"code","execution_count":null,"id":"db779d91-f02d-4d0c-ad97-4eb3fcba982b","metadata":{"id":"db779d91-f02d-4d0c-ad97-4eb3fcba982b"},"outputs":[],"source":["class EncoderLayer(nn.Module):\n","    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n","        super(EncoderLayer, self).__init__()\n","        self.self_attn = MultiHeadAttention(d_model, num_heads)\n","        self.ff = FeedForward(d_model, d_ff)\n","        self.norm1 = nn.LayerNorm(d_model)\n","        self.norm2 = nn.LayerNorm(d_model)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, mask=None):\n","        attn_output = self.self_attn(x, x, x, mask)\n","        x = self.norm1(x + self.dropout(attn_output))\n","        ff_output = self.ff(x)\n","        x = self.norm2(x + self.dropout(ff_output))\n","        return x"]},{"cell_type":"markdown","id":"e922f52e-66a5-4298-ab35-3d9f8e340e56","metadata":{"id":"e922f52e-66a5-4298-ab35-3d9f8e340e56"},"source":["## Embedding layer + Positional Embedding"]},{"cell_type":"code","execution_count":null,"id":"10a7bf64-bb8c-409b-b7c1-7e59fa774983","metadata":{"id":"10a7bf64-bb8c-409b-b7c1-7e59fa774983"},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(self, d_model, num_heads, d_ff, num_layers, vocab_size, max_seq_len, dropout=0.1):\n","        super(Encoder, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, d_model)\n","        self.pos_encoding = self._generate_positional_encoding(max_seq_len, d_model)\n","        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def _generate_positional_encoding(self, max_seq_len, d_model):\n","        pos = torch.arange(max_seq_len).unsqueeze(1)\n","        i = torch.arange(d_model).unsqueeze(0)\n","        angle_rates = 1 / torch.pow(10000, (2 * (i // 2)) / torch.tensor(d_model, dtype=torch.float32))\n","        pos_encoding = pos * angle_rates\n","        pos_encoding[:, 0::2] = torch.sin(pos_encoding[:, 0::2])\n","        pos_encoding[:, 1::2] = torch.cos(pos_encoding[:, 1::2])\n","        return pos_encoding.unsqueeze(0)\n","\n","    def forward(self, x, mask=None):\n","        seq_len = x.size(1)\n","        x = self.embedding(x)\n","        x = x + self.pos_encoding[:, :seq_len, :]\n","        x = self.dropout(x)\n","        for layer in self.layers:\n","            x = layer(x, mask)\n","        return x"]},{"cell_type":"markdown","id":"0de7dd11-22ba-4524-bb63-2c37f0ba96b7","metadata":{"id":"0de7dd11-22ba-4524-bb63-2c37f0ba96b7"},"source":["# Decoder"]},{"cell_type":"code","execution_count":null,"id":"41198f8d-1ce6-40df-8d31-2f79960df48b","metadata":{"id":"41198f8d-1ce6-40df-8d31-2f79960df48b"},"outputs":[],"source":["class DecoderLayer(nn.Module):\n","    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n","        super(DecoderLayer, self).__init__()\n","        self.self_attn = MultiHeadAttention(d_model, num_heads)\n","        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n","        self.ff = FeedForward(d_model, d_ff)\n","        self.norm1 = nn.LayerNorm(d_model)\n","        self.norm2 = nn.LayerNorm(d_model)\n","        self.norm3 = nn.LayerNorm(d_model)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, enc_output, tgt_mask=None, memory_mask=None):\n","        self_attn_output = self.self_attn(x, x, x, tgt_mask)\n","        x = self.norm1(x + self.dropout(self_attn_output))\n","\n","        enc_dec_attn_output = self.enc_dec_attn(x, enc_output, enc_output, memory_mask)\n","        x = self.norm2(x + self.dropout(enc_dec_attn_output))\n","\n","        ff_output = self.ff(x)\n","        x = self.norm3(x + self.dropout(ff_output))\n","        return x"]},{"cell_type":"code","execution_count":null,"id":"6b5f6e46-ac87-454d-a27b-835888dcfedf","metadata":{"id":"6b5f6e46-ac87-454d-a27b-835888dcfedf"},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self, d_model, num_heads, d_ff, num_layers, vocab_size, max_seq_len, dropout=0.1):\n","        super(Decoder, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, d_model)\n","        self.pos_encoding = self._generate_positional_encoding(max_seq_len, d_model)\n","        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def _generate_positional_encoding(self, max_seq_len, d_model):\n","        pos = torch.arange(max_seq_len).unsqueeze(1)\n","        i = torch.arange(d_model).unsqueeze(0)\n","        angle_rates = 1 / torch.pow(10000, (2 * (i // 2)) / torch.tensor(d_model, dtype=torch.float32))\n","        pos_encoding = pos * angle_rates\n","        pos_encoding[:, 0::2] = torch.sin(pos_encoding[:, 0::2])\n","        pos_encoding[:, 1::2] = torch.cos(pos_encoding[:, 1::2])\n","        return pos_encoding.unsqueeze(0)\n","\n","    def forward(self, x, enc_output, tgt_mask=None, memory_mask=None):\n","        seq_len = x.size(1)\n","        x = self.embedding(x)\n","        x = x + self.pos_encoding[:, :seq_len, :]\n","        x = self.dropout(x)\n","        for layer in self.layers:\n","            x = layer(x, enc_output, tgt_mask, memory_mask)\n","        return x"]},{"cell_type":"markdown","id":"bfdb0626-09d2-4f7b-a5c3-4649e60a26af","metadata":{"id":"bfdb0626-09d2-4f7b-a5c3-4649e60a26af"},"source":["# Put it all together"]},{"cell_type":"markdown","id":"0c1635c9-1f5d-4e75-8682-0adc397ec438","metadata":{"id":"0c1635c9-1f5d-4e75-8682-0adc397ec438"},"source":["## Transformer"]},{"cell_type":"code","execution_count":null,"id":"13db8867-0bb9-408c-b4ae-8f283542da42","metadata":{"id":"13db8867-0bb9-408c-b4ae-8f283542da42"},"outputs":[],"source":["class Transformer(nn.Module):\n","    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, d_ff, num_layers, max_seq_len, dropout=0.1):\n","        super(Transformer, self).__init__()\n","        self.encoder = Encoder(d_model, num_heads, d_ff, num_layers, src_vocab_size, max_seq_len, dropout)\n","        self.decoder = Decoder(d_model, num_heads, d_ff, num_layers, tgt_vocab_size, max_seq_len, dropout)\n","        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n","\n","    def forward(self, src, tgt, src_mask=None, tgt_mask=None, memory_mask=None):\n","        enc_output = self.encoder(src, src_mask)\n","        dec_output = self.decoder(tgt, enc_output, tgt_mask, memory_mask)\n","        output = self.fc_out(dec_output)\n","        return output"]},{"cell_type":"markdown","id":"ba2838a0-2c21-4c66-8a61-2ff355c60677","metadata":{"id":"ba2838a0-2c21-4c66-8a61-2ff355c60677"},"source":["## Masks"]},{"cell_type":"code","execution_count":null,"id":"daac7ab9-f5e2-4b97-9f7e-0c401e1476e6","metadata":{"id":"daac7ab9-f5e2-4b97-9f7e-0c401e1476e6"},"outputs":[],"source":["def create_src_mask(src, pad_idx):\n","    src_mask = (src != pad_idx).unsqueeze(-2)\n","    return src_mask\n","\n","def create_tgt_mask(tgt, pad_idx):\n","    tgt_pad_mask = (tgt != pad_idx).unsqueeze(-2)\n","    tgt_len = tgt.size(1)\n","    tgt_sub_mask = torch.tril(torch.ones((tgt_len, tgt_len), device=tgt.device)).bool()\n","    tgt_mask = tgt_pad_mask & tgt_sub_mask\n","    return tgt_mask"]},{"cell_type":"code","execution_count":null,"id":"f96a7928-6c80-4e42-aedb-0ac01ade2ff2","metadata":{"id":"f96a7928-6c80-4e42-aedb-0ac01ade2ff2"},"outputs":[],"source":["src_vocab_size = 10000\n","tgt_vocab_size = 10000\n","d_model = 512\n","num_heads = 8\n","d_ff = 2048\n","num_layers = 6\n","max_seq_len = 100\n","dropout = 0.1\n","pad_idx = 0"]},{"cell_type":"code","execution_count":null,"id":"f11284d7-3a4e-46ff-9229-ddd59ca70d1b","metadata":{"id":"f11284d7-3a4e-46ff-9229-ddd59ca70d1b","outputId":"add08a9a-f93b-4eef-c423-5df5d02b08a1"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([32, 100]) torch.Size([32, 100])\n","torch.Size([32, 100, 10000])\n"]}],"source":["transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, d_ff, num_layers, max_seq_len, dropout)\n","src = torch.randint(0, src_vocab_size, (32, max_seq_len))\n","tgt = torch.randint(0, tgt_vocab_size, (32, max_seq_len))\n","print(src.shape, tgt.shape)\n","src_mask = create_src_mask(src, pad_idx)\n","tgt_mask = create_tgt_mask(tgt, pad_idx)\n","output = transformer(src, tgt, src_mask, tgt_mask)\n","\n","print(output.shape)"]},{"cell_type":"markdown","id":"89899149-4ae1-4d72-a37c-fbb582ca16dc","metadata":{"id":"89899149-4ae1-4d72-a37c-fbb582ca16dc"},"source":["## Tokenization"]},{"cell_type":"markdown","id":"e1011580-fa0a-477d-8ce8-41bb39d27f58","metadata":{"id":"e1011580-fa0a-477d-8ce8-41bb39d27f58"},"source":["Step to convert text into words.\n","\n","Two simplest approches are:\n","\n","| Method              | Vocab Size | Sequence lengths |\n","| :---------------- | :------: | ----: |\n","| Number for each char        |   Small   | Very long |\n","| Number for each word           |   Very Large   | Contained (same as text) |"]},{"cell_type":"markdown","id":"71344487-196e-40d0-961d-c3aaa47edf26","metadata":{"id":"71344487-196e-40d0-961d-c3aaa47edf26"},"source":["Modern LLM use peicewise encoders, which are somewhere in between these two approches. Exact working is out of the scope of this lecture. Two popular approches are:-\n","+ [Byte Pair Tokenization](https://www.youtube.com/watch?v=HEikzVL-lZU)\n","+ [WordPiece Tokenization](https://www.youtube.com/watch?v=qpv6ms_t_1A)"]},{"cell_type":"markdown","id":"ef6b4f2f-ce0b-41be-94d6-f3188960c467","metadata":{"id":"ef6b4f2f-ce0b-41be-94d6-f3188960c467"},"source":["# Why is transformer so revolutionary?"]},{"cell_type":"markdown","id":"1660eb1d-f784-49ba-a190-44830b8fb83d","metadata":{"id":"1660eb1d-f784-49ba-a190-44830b8fb83d"},"source":["+ Very efficent compute wise\n","+ [The Bitter Lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html) ==> Transformer is one of the greatest examples of this.\n","+ Generalizable across domains"]},{"cell_type":"markdown","id":"59339592-4285-496d-a370-552d1493f58f","metadata":{"id":"59339592-4285-496d-a370-552d1493f58f"},"source":["# Limitations of transformers"]},{"cell_type":"markdown","id":"be8ec5d6-a960-49e8-ab95-ab964edff18f","metadata":{"id":"be8ec5d6-a960-49e8-ab95-ab964edff18f"},"source":["+ High memory usage\n","+ Large compute and data requirements\n","+ Limitations of token lengths"]},{"cell_type":"markdown","id":"5cd10cd6-59e5-4640-8186-710fcedd7c5b","metadata":{"id":"5cd10cd6-59e5-4640-8186-710fcedd7c5b"},"source":["# How it leads to fancy stuff like BERT and GPT?"]},{"cell_type":"markdown","id":"b9ccc21f-dfe4-445f-8f0d-c79700624347","metadata":{"id":"b9ccc21f-dfe4-445f-8f0d-c79700624347"},"source":["<div style=\"text-align:center\">\n","  <img src=\"https://miro.medium.com/v2/resize:fit:434/1*D5xg0yz7YzBSzS_F1efLAA.png\" width=\"500px\"/>\n","</div>"]},{"cell_type":"code","execution_count":null,"id":"de3780c4-5582-43f0-8696-e77dced89480","metadata":{"id":"de3780c4-5582-43f0-8696-e77dced89480"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"py39","language":"python","name":"py39"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}