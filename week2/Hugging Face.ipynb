{"cells":[{"cell_type":"markdown","id":"d7f9ceec-2428-4561-bc54-2a2bdebfc185","metadata":{"id":"d7f9ceec-2428-4561-bc54-2a2bdebfc185"},"source":["<h1 style=\"font-size:60px;\"><center>ðŸ¤— Hugging Face</center></h1>"]},{"cell_type":"markdown","id":"00e24076-37d3-4615-8f45-5abea5113103","metadata":{"id":"00e24076-37d3-4615-8f45-5abea5113103"},"source":["# WHY?"]},{"cell_type":"markdown","id":"7123912f-7d2e-4e00-aced-41519501d9f4","metadata":{"id":"7123912f-7d2e-4e00-aced-41519501d9f4"},"source":["## 2018 - Aam zindagi"]},{"cell_type":"code","execution_count":null,"id":"bdb97b24-4d00-4b07-bcec-d06440bfa7ea","metadata":{"id":"bdb97b24-4d00-4b07-bcec-d06440bfa7ea"},"outputs":[],"source":["class PaddingInputExample(object):\n","    \"\"\"Fake example so the num input examples is a multiple of the batch size.\n","  When running eval/predict on the TPU, we need to pad the number of examples\n","  to be a multiple of the batch size, because the TPU requires a fixed batch\n","  size. The alternative is to drop the last batch, which is bad because it means\n","  the entire output data won't be generated.\n","  We use this class instead of `None` because treating `None` as padding\n","  battches could cause silent errors.\n","  \"\"\"\n","\n","class InputExample(object):\n","    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n","\n","    def __init__(self, guid, text_a, text_b=None, label=None):\n","        \"\"\"Constructs a InputExample.\n","    Args:\n","      guid: Unique id for the example.\n","      text_a: string. The untokenized text of the first sequence. For single\n","        sequence tasks, only this sequence must be specified.\n","      text_b: (Optional) string. The untokenized text of the second sequence.\n","        Only must be specified for sequence pair tasks.\n","      label: (Optional) string. The label of the example. This should be\n","        specified for train and dev examples, but not for test examples.\n","    \"\"\"\n","        self.guid = guid\n","        self.text_a = text_a\n","        self.text_b = text_b\n","        self.label = label\n","\n","def create_tokenizer_from_hub_module():\n","    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n","    bert_module =  hub.Module(bert_path)\n","    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n","    vocab_file, do_lower_case = sess.run(\n","        [\n","            tokenization_info[\"vocab_file\"],\n","            tokenization_info[\"do_lower_case\"],\n","        ]\n","    )\n","\n","    return FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n","\n","def convert_single_example(tokenizer, example, max_seq_length=256):\n","    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n","\n","    if isinstance(example, PaddingInputExample):\n","        input_ids = [0] * max_seq_length\n","        input_mask = [0] * max_seq_length\n","        segment_ids = [0] * max_seq_length\n","        label = 0\n","        return input_ids, input_mask, segment_ids, label\n","\n","    tokens_a = tokenizer.tokenize(example.text_a)\n","    if len(tokens_a) > max_seq_length - 2:\n","        tokens_a = tokens_a[0 : (max_seq_length - 2)]\n","\n","    tokens = []\n","    segment_ids = []\n","    tokens.append(\"[CLS]\")\n","    segment_ids.append(0)\n","    for token in tokens_a:\n","        tokens.append(token)\n","        segment_ids.append(0)\n","    tokens.append(\"[SEP]\")\n","    segment_ids.append(0)\n","\n","    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n","    # tokens are attended to.\n","    input_mask = [1] * len(input_ids)\n","\n","    # Zero-pad up to the sequence length.\n","    while len(input_ids) < max_seq_length:\n","        input_ids.append(0)\n","        input_mask.append(0)\n","        segment_ids.append(0)\n","\n","    assert len(input_ids) == max_seq_length\n","    assert len(input_mask) == max_seq_length\n","    assert len(segment_ids) == max_seq_length\n","\n","    return input_ids, input_mask, segment_ids, example.label\n","\n","def convert_examples_to_features(tokenizer, examples, max_seq_length=256):\n","    \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n","\n","    input_ids, input_masks, segment_ids, labels = [], [], [], []\n","    for example in tqdm_notebook(examples, desc=\"Converting examples to features\"):\n","        input_id, input_mask, segment_id, label = convert_single_example(\n","            tokenizer, example, max_seq_length\n","        )\n","        input_ids.append(input_id)\n","        input_masks.append(input_mask)\n","        segment_ids.append(segment_id)\n","        labels.append(label)\n","    return (\n","        np.array(input_ids),\n","        np.array(input_masks),\n","        np.array(segment_ids),\n","        np.array(labels).reshape(-1, 1),\n","    )\n","\n","def convert_text_to_examples(texts, labels):\n","    \"\"\"Create InputExamples\"\"\"\n","    InputExamples = []\n","    for text, label in zip(texts, labels):\n","        InputExamples.append(\n","            InputExample(guid=None, text_a=\" \".join(text), text_b=None, label=label)\n","        )\n","    return InputExamples\n","\n","# Instantiate tokenizer\n","tokenizer = create_tokenizer_from_hub_module()\n","\n","# Convert data to InputExample format\n","train_examples = convert_text_to_examples(train_text, trainY)\n","test_examples = convert_text_to_examples(test_text, testY)\n","\n","# Convert to features\n","(train_input_ids, train_input_masks, train_segment_ids, trainY\n",") = convert_examples_to_features(tokenizer, train_examples, max_seq_length=max_seq_length)\n","(test_input_ids, test_input_masks, test_segment_ids, testY\n",") = convert_examples_to_features(tokenizer, test_examples, max_seq_length=max_seq_length)\n","\n","\n","class BertLayer(tf.layers.Layer):\n","    def __init__(self, n_fine_tune_layers=10, **kwargs):\n","        self.n_fine_tune_layers = n_fine_tune_layers\n","        self.trainable = True\n","        self.output_size = 768\n","        super(BertLayer, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        self.bert = hub.Module(\n","            bert_path,\n","            trainable=self.trainable,\n","            name=\"{}_module\".format(self.name)\n","        )\n","\n","        trainable_vars = self.bert.variables\n","\n","        # Remove unused layers\n","        trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n","\n","        # Select how many layers to fine tune\n","        trainable_vars = trainable_vars[-self.n_fine_tune_layers :]\n","\n","        # Add to trainable weights\n","        for var in trainable_vars:\n","            self._trainable_weights.append(var)\n","\n","        for var in self.bert.variables:\n","            if var not in self._trainable_weights:\n","                self._non_trainable_weights.append(var)\n","\n","        super(BertLayer, self).build(input_shape)\n","\n","    def call(self, inputs):\n","        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n","        input_ids, input_mask, segment_ids = inputs\n","        bert_inputs = dict(\n","            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n","        )\n","        result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n","            \"pooled_output\"\n","        ]\n","        return result\n","\n","    def compute_output_shape(self, input_shape):\n","        return (input_shape[0], self.output_size)\n","\n","def initialize_vars(sess):\n","    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) #command to run codeon multiple gpu\n","    sess.run(tf.local_variables_initializer())\n","    sess.run(tf.global_variables_initializer())\n","    sess.run(tf.tables_initializer())\n","    K.set_session(sess)\n","\n","\n","\n","#text model\n","def news_model(x_train, y_train, x_val, y_val, params):\n","\n","#     pprint(params)\n","    try:\n","        del model\n","    except:\n","        pass\n","    K.clear_session()\n","    gc.collect()\n","\n","    with tf.device('/cpu:0'):\n","        bert_base = BertLayer()\n","        bert_base.trainable= params['bert_trainable']\n","\n","        in_id = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n","        in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n","        in_segment = tf.keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\n","        bert_inputs = [in_id, in_mask, in_segment]\n","        bert_output = bert_base(bert_inputs)\n","\n","        if params['text_no_hidden_layer']>0:\n","            for i in range(params['text_no_hidden_layer']):\n","                bert_output = tf.keras.layers.Dense(params['text_hidden_neurons'], activation='relu')(bert_output)\n","                bert_output = tf.keras.layers.Dropout(params['dropout'])(bert_output)\n","\n","        text_repr = tf.keras.layers.Dense(params['repr_size'], activation='relu')(bert_output)\n","\n","        #image model\n","        conv_base = tf.keras.applications.VGG19(weights='imagenet', include_top=False, input_shape=(3,224,224))\n","        conv_base.trainable=False\n","#         conv_base = base\n","\n","        input_image = tf.keras.layers.Input(shape=(3,224,224))\n","        base_output = conv_base(input_image)\n","        flat = tf.keras.layers.Flatten()(base_output)\n","\n","        if params['vis_no_hidden_layer']>0:\n","            for i in range(params['vis_no_hidden_layer']):\n","                flat = tf.keras.layers.Dense(params['vis_hidden_neurons'], activation='relu')(flat)\n","                flat = tf.keras.layers.Dropout(params['dropout'])(flat)\n","\n","        visual_repr = tf.keras.layers.Dense(params['repr_size'],activation='relu')(flat)\n","\n","\n","        #classifier\n","        combine_repr = tf.keras.layers.concatenate([text_repr, visual_repr])\n","        com_drop=tf.keras.layers.Dropout(params['dropout'])(combine_repr)\n","\n","        if params['final_no_hidden_layer']>0:\n","            for i in range(params['final_no_hidden_layer']):\n","                com_drop = tf.keras.layers.Dense(params['final_hidden_neurons'], activation='relu')(com_drop)\n","                com_drop=tf.keras.layers.Dropout(params['dropout'])(com_drop)\n","\n","        prediction = tf.keras.layers.Dense(1,activation='sigmoid')(com_drop)\n","\n","        model = tf.keras.models.Model(inputs=[in_id,in_mask,in_segment,input_image], outputs=prediction)\n","\n","    model = tf.keras.utils.multi_gpu_model(model,gpus=4)\n","\n","\n","    model.compile(loss='binary_crossentropy', optimizer=params['optimizer'](lr=params['lr']), metrics=['accuracy'])\n","    initialize_vars(sess)\n","\n","    out = model.fit(x_train, y_train,\n","                    batch_size=params['batch_size'],\n","                    epochs=params['epochs'],\n","                    verbose=0,\n","                    shuffle=True,\n","                    validation_data=[x_val, y_val],callbacks=[live()])\n","\n","    return out, model"]},{"cell_type":"markdown","id":"c0ed844a-3164-4f2f-8ec1-f5e7628e756d","metadata":{"id":"c0ed844a-3164-4f2f-8ec1-f5e7628e756d"},"source":["## Now -  Mentos zindagi"]},{"cell_type":"code","execution_count":null,"id":"bc609ee6-4bd5-417d-8253-e9f41249197b","metadata":{"id":"bc609ee6-4bd5-417d-8253-e9f41249197b"},"outputs":[],"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","def tokenize_function(examples):\n","    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)\n","\n","model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)"]},{"cell_type":"markdown","id":"f7877878-4103-40f2-9ca9-0c1df3bdb0b6","metadata":{"id":"f7877878-4103-40f2-9ca9-0c1df3bdb0b6"},"source":["<div style=\"text-align:center\"><img src=\"https://media1.tenor.com/m/gcB9oM_fwX4AAAAC/sabbir31x-waqt-badal-diye.gif\" /></div>"]},{"cell_type":"markdown","id":"687d2fc6-5907-4024-9df2-883f53982422","metadata":{"id":"687d2fc6-5907-4024-9df2-883f53982422"},"source":["# Tokenizers"]},{"cell_type":"code","execution_count":null,"id":"ee2e1f03-3558-4447-9dfa-34ef70e09d33","metadata":{"id":"ee2e1f03-3558-4447-9dfa-34ef70e09d33","outputId":"1f3097cb-ce6f-4552-8451-243c3f721ffd"},"outputs":[{"name":"stdout","output_type":"stream","text":["['This', 'is', 'a', 'sentance']\n"]}],"source":["tokenized_text = \"This is a sentance\".split()\n","print(tokenized_text)"]},{"cell_type":"markdown","id":"f3360cbb-0f28-4a14-9c5a-5a1cae81f96f","metadata":{"id":"f3360cbb-0f28-4a14-9c5a-5a1cae81f96f"},"source":["SubWord Tokenization :\n","+ fixing vocabulary size\n","+ learnt from a large corpus by computing the most frequently occuring strings/sub-strings\n","+ express any string as a a combination of the vocab items\n","+ Multiple Algos : Byte Pair Encoding, SentencePiece\n","\n","[Byte Pair Tokenization](https://www.youtube.com/watch?v=HEikzVL-lZU)\n","\n","[WordPiece Tokenization](https://www.youtube.com/watch?v=qpv6ms_t_1A)"]},{"cell_type":"code","execution_count":null,"id":"554ff72c-55e9-4bab-80c6-207c4070ef7e","metadata":{"id":"554ff72c-55e9-4bab-80c6-207c4070ef7e"},"outputs":[],"source":["from transformers import BertTokenizer\n","\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"]},{"cell_type":"code","execution_count":null,"id":"3b095466-8adf-436f-a033-04716e09d76f","metadata":{"id":"3b095466-8adf-436f-a033-04716e09d76f"},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"]},{"cell_type":"code","execution_count":null,"id":"e54b83f0-b6c8-43fe-b4ab-ccbb04748957","metadata":{"id":"e54b83f0-b6c8-43fe-b4ab-ccbb04748957","outputId":"6941ae8f-c250-4046-cb65-a14b746b0cd6"},"outputs":[{"data":{"text/plain":["{'input_ids': [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer(\"Using a Transformer network is simple\")"]},{"cell_type":"code","execution_count":null,"id":"0bd573d6-f6db-4b98-8b44-7e823b816d84","metadata":{"id":"0bd573d6-f6db-4b98-8b44-7e823b816d84","outputId":"3a5fb1e6-23da-4e35-83ea-8f5780d4d90f"},"outputs":[{"data":{"text/plain":["['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple']"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["tokens = tokenizer.tokenize(\"Using a Transformer network is simple\")\n","tokens"]},{"cell_type":"code","execution_count":null,"id":"04c82a6c-1a9a-4405-8de8-92884e2ba0b3","metadata":{"id":"04c82a6c-1a9a-4405-8de8-92884e2ba0b3","outputId":"c431231d-29e0-45dd-b9a2-cb8a2f2767b5"},"outputs":[{"data":{"text/plain":["[7993, 170, 13809, 23763, 2443, 1110, 3014]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.convert_tokens_to_ids(tokens)\n"]},{"cell_type":"code","execution_count":null,"id":"403b671d-cdcb-449d-af8e-3c9a4e402a91","metadata":{"id":"403b671d-cdcb-449d-af8e-3c9a4e402a91","outputId":"1328bc23-91b1-49a2-c61f-9501020f1c4d"},"outputs":[{"data":{"text/plain":["'Using a Transformer network is simple'"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["decoded_string = tokenizer.decode([7993, 170, 13809, 23763, 2443, 1110, 3014])\n","decoded_string"]},{"cell_type":"markdown","id":"e013ff49-c7ae-427e-be7e-a4301dbdcc42","metadata":{"id":"e013ff49-c7ae-427e-be7e-a4301dbdcc42"},"source":["# Inference"]},{"cell_type":"code","execution_count":null,"id":"c4592974-9285-4fa8-9f16-fe9cd04c2e1a","metadata":{"id":"c4592974-9285-4fa8-9f16-fe9cd04c2e1a","outputId":"0385be08-68e1-4634-80d6-6a79f0f6baf9"},"outputs":[{"name":"stderr","output_type":"stream","text":["No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n","Using a pipeline without specifying a model name and revision in production is not recommended.\n"]},{"data":{"text/plain":["[{'label': 'POSITIVE', 'score': 0.9824662208557129},\n"," {'label': 'NEGATIVE', 'score': 0.9996127486228943}]"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import pipeline\n","\n","classifier = pipeline(\"sentiment-analysis\")\n","classifier(\n","    [\n","        \"IIIT H has a beutiful campus.\",\n","        \"Too humid.\",\n","    ]\n",")"]},{"cell_type":"code","execution_count":null,"id":"12e0d32d-8d7a-4744-b9d9-17f5cec99791","metadata":{"id":"12e0d32d-8d7a-4744-b9d9-17f5cec99791","outputId":"2712a5e4-0c1b-4247-d678-f357e100d414"},"outputs":[{"data":{"text/plain":["[{'label': 'POSITIVE', 'score': 0.9824662208557129},\n"," {'label': 'NEGATIVE', 'score': 0.9996127486228943}]"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import pipeline\n","\n","classifier = pipeline(\"sentiment-analysis\", model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\n","classifier(\n","    [\n","        \"IIIT H has a beutiful campus.\",\n","        \"Too humid.\",\n","    ]\n",")"]},{"cell_type":"markdown","id":"a3de2bc1-aba1-4e02-9f5f-513c34776b79","metadata":{"id":"a3de2bc1-aba1-4e02-9f5f-513c34776b79"},"source":["## what did it do?"]},{"cell_type":"code","execution_count":null,"id":"1aacea0e-0104-470c-a7da-058910fcf273","metadata":{"id":"1aacea0e-0104-470c-a7da-058910fcf273"},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)"]},{"cell_type":"code","execution_count":null,"id":"1344eeb3-020c-4228-a3c2-7edc1d91a484","metadata":{"id":"1344eeb3-020c-4228-a3c2-7edc1d91a484","outputId":"338c6f50-b2da-4a6d-bba0-0f6b41bcb95d"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'input_ids': tensor([[  101,  3523,  2102,  1044,  2038,  1037,  2022, 21823,  3993,  3721,\n","          1012,   102],\n","        [  101,  2205, 14178,  1012,   102,     0,     0,     0,     0,     0,\n","             0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]])}\n"]}],"source":["raw_inputs = [\n","        \"IIIT H has a beutiful campus.\",\n","        \"Too humid.\"]\n","inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n","print(inputs)"]},{"cell_type":"code","execution_count":null,"id":"501b3d05-ed6e-4ce1-8cd6-f7a5262d70fb","metadata":{"id":"501b3d05-ed6e-4ce1-8cd6-f7a5262d70fb"},"outputs":[],"source":["from transformers import AutoModel\n","\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","model = AutoModel.from_pretrained(checkpoint)"]},{"cell_type":"code","execution_count":null,"id":"21dcaae7-bba4-4ab9-9f5f-83ef8db6a268","metadata":{"id":"21dcaae7-bba4-4ab9-9f5f-83ef8db6a268","outputId":"9eec96f6-e529-4f27-a55e-070803f76365"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([2, 12, 768])\n"]}],"source":["outputs = model(**inputs)\n","print(outputs.last_hidden_state.shape)"]},{"cell_type":"code","execution_count":null,"id":"f057d603-5eff-4af9-8c59-00d85d523226","metadata":{"id":"f057d603-5eff-4af9-8c59-00d85d523226"},"outputs":[],"source":["from transformers import AutoModelForSequenceClassification\n","\n","checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n","outputs = model(**inputs)"]},{"cell_type":"code","execution_count":null,"id":"d804061d-53fb-4893-8e3a-dabbdb5e537b","metadata":{"id":"d804061d-53fb-4893-8e3a-dabbdb5e537b","outputId":"1811fc69-0155-422e-b981-ca8204bfe0a2"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([2, 2])\n"]}],"source":["print(outputs.logits.shape)"]},{"cell_type":"code","execution_count":null,"id":"a3e0f6df-f9cb-425d-8782-fe23e686d443","metadata":{"id":"a3e0f6df-f9cb-425d-8782-fe23e686d443","outputId":"446804e7-5000-4791-ea86-2dda34a99590"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[-1.9519,  2.0740],\n","        [ 4.2943, -3.5618]], grad_fn=<AddmmBackward0>)\n"]}],"source":["print(outputs.logits)"]},{"cell_type":"code","execution_count":null,"id":"15c2f8f7-fafc-49b4-a5d0-bb6ca400dc0b","metadata":{"id":"15c2f8f7-fafc-49b4-a5d0-bb6ca400dc0b","outputId":"b49caa8c-a49a-4ddb-a019-a0859758297c"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[1.7534e-02, 9.8247e-01],\n","        [9.9961e-01, 3.8726e-04]], grad_fn=<SoftmaxBackward0>)\n"]}],"source":["import torch\n","\n","predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n","print(predictions)"]},{"cell_type":"code","execution_count":null,"id":"ae8d010a-fd23-4cd4-84ea-4312cf0fd4f9","metadata":{"id":"ae8d010a-fd23-4cd4-84ea-4312cf0fd4f9","outputId":"8ed9c6fe-3371-44f9-bf67-cf5c92d0c448"},"outputs":[{"data":{"text/plain":["tensor([1, 0])"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["torch.argmax(predictions,axis=0)"]},{"cell_type":"code","execution_count":null,"id":"7ba3eaeb-4a69-4be9-afa7-f80d644a9c55","metadata":{"id":"7ba3eaeb-4a69-4be9-afa7-f80d644a9c55","outputId":"c60b9fa5-3ed0-4750-f92b-ffeb48a35051"},"outputs":[{"data":{"text/plain":["{0: 'NEGATIVE', 1: 'POSITIVE'}"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["model.config.id2label"]},{"cell_type":"markdown","id":"50cac46c-e47d-4cb6-8b46-646fa34abb08","metadata":{"id":"50cac46c-e47d-4cb6-8b46-646fa34abb08"},"source":["# Writing your own Training loop"]},{"cell_type":"code","execution_count":null,"id":"9ea00063-dfbc-494c-a522-71f617ea9e7d","metadata":{"id":"9ea00063-dfbc-494c-a522-71f617ea9e7d","outputId":"ed01ce3e-b2fc-491d-ae8c-330a0bb25c47","colab":{"referenced_widgets":["00a039d234e5470bb5441578ea0375b4"]}},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"00a039d234e5470bb5441578ea0375b4","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/1725 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from datasets import load_dataset\n","from transformers import AutoTokenizer, DataCollatorWithPadding\n","\n","raw_datasets = load_dataset(\"glue\", \"mrpc\")\n","checkpoint = \"bert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","\n","\n","def tokenize_function(example):\n","    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n","\n","\n","tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":null,"id":"3d1adccc-84a0-4f2c-be5c-fbc663494003","metadata":{"id":"3d1adccc-84a0-4f2c-be5c-fbc663494003","outputId":"57f36e42-9b56-4e50-9506-bba6dc758b89"},"outputs":[{"data":{"text/plain":["['labels', 'input_ids', 'token_type_ids', 'attention_mask']"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n","tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n","tokenized_datasets.set_format(\"torch\")\n","tokenized_datasets[\"train\"].column_names"]},{"cell_type":"code","execution_count":null,"id":"09d1b931-a507-4edf-bd63-f7d55c98b213","metadata":{"id":"09d1b931-a507-4edf-bd63-f7d55c98b213"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","\n","train_dataloader = DataLoader(\n","    tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n",")\n","eval_dataloader = DataLoader(\n","    tokenized_datasets[\"validation\"], batch_size=8, collate_fn=data_collator\n",")"]},{"cell_type":"code","execution_count":null,"id":"98adc08b-98f6-4cc2-9e3a-c8d794ab8aaa","metadata":{"id":"98adc08b-98f6-4cc2-9e3a-c8d794ab8aaa","outputId":"d5ac8742-08d8-47ee-dfef-32e191dc5326"},"outputs":[{"data":{"text/plain":["{'labels': torch.Size([8]),\n"," 'input_ids': torch.Size([8, 68]),\n"," 'token_type_ids': torch.Size([8, 68]),\n"," 'attention_mask': torch.Size([8, 68])}"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["for batch in train_dataloader:\n","    break\n","{k: v.shape for k, v in batch.items()}"]},{"cell_type":"code","execution_count":null,"id":"2a483e5c-20b5-4ffe-b0e5-5fb80c5efbb7","metadata":{"id":"2a483e5c-20b5-4ffe-b0e5-5fb80c5efbb7","outputId":"aee25089-01ac-4a37-aa68-db8659324468"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from transformers import AutoModelForSequenceClassification\n","\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"]},{"cell_type":"code","execution_count":null,"id":"04958bb1-f519-42c9-905b-49990e6ccd74","metadata":{"id":"04958bb1-f519-42c9-905b-49990e6ccd74","outputId":"0c824011-41cf-4a42-c131-9da9cd8dd768"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(0.6938, grad_fn=<NllLossBackward0>) torch.Size([8, 2])\n"]}],"source":["outputs = model(**batch)\n","print(outputs.loss, outputs.logits.shape)"]},{"cell_type":"code","execution_count":null,"id":"66b9b8bb-ee48-4559-8509-6eba10f00937","metadata":{"id":"66b9b8bb-ee48-4559-8509-6eba10f00937","outputId":"402b8ea9-88a3-4802-cc03-fca23caa4f19"},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/hitkul/miniconda3/envs/mix/lib/python3.9/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}],"source":["from transformers import AdamW\n","\n","optimizer = AdamW(model.parameters(), lr=5e-5)"]},{"cell_type":"code","execution_count":null,"id":"e2423a49-058c-44ce-811f-d548e6bd3428","metadata":{"id":"e2423a49-058c-44ce-811f-d548e6bd3428","outputId":"e95d5128-78ce-4f1e-f5ba-c5fae5af0a1f"},"outputs":[{"name":"stdout","output_type":"stream","text":["459\n"]}],"source":["from transformers import get_scheduler\n","\n","num_epochs = 1\n","num_training_steps = num_epochs * len(train_dataloader)\n","lr_scheduler = get_scheduler(\n","    \"linear\",\n","    optimizer=optimizer,\n","    num_warmup_steps=0,\n","    num_training_steps=num_training_steps,\n",")\n","print(num_training_steps)"]},{"cell_type":"code","execution_count":null,"id":"a75f195b-7783-4d37-a8d7-52b87b406802","metadata":{"id":"a75f195b-7783-4d37-a8d7-52b87b406802","outputId":"6afbd543-ca33-4790-8c6b-8aecf20d2fc3"},"outputs":[{"data":{"text/plain":["device(type='cpu')"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","model.to(device)\n","device"]},{"cell_type":"code","execution_count":null,"id":"5ed7ad4d-fc51-4371-a313-e9b658512a18","metadata":{"id":"5ed7ad4d-fc51-4371-a313-e9b658512a18","outputId":"6761624b-df28-4e7e-e5f9-dd4a08ef6a82","colab":{"referenced_widgets":["8cf90b2e65184a669d534444d5e26c5f"]}},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8cf90b2e65184a669d534444d5e26c5f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/459 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from tqdm.auto import tqdm\n","\n","progress_bar = tqdm(range(num_training_steps))\n","\n","model.train()\n","for epoch in range(num_epochs):\n","    for batch in train_dataloader:\n","        batch = {k: v.to(device) for k, v in batch.items()}\n","        outputs = model(**batch)\n","        loss = outputs.loss\n","        loss.backward()\n","\n","        optimizer.step()\n","        lr_scheduler.step()\n","        optimizer.zero_grad()\n","        progress_bar.update(1)"]},{"cell_type":"code","execution_count":null,"id":"dea44f64-de1d-4ac6-baf7-8c4b4d5ff60b","metadata":{"id":"dea44f64-de1d-4ac6-baf7-8c4b4d5ff60b","outputId":"141cd3b8-96e1-4b12-d167-97c1fa4c7db2"},"outputs":[{"data":{"text/plain":["{'accuracy': 0.8284313725490197, 'f1': 0.875886524822695}"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["import evaluate\n","\n","metric = evaluate.load(\"glue\", \"mrpc\")\n","model.eval()\n","for batch in eval_dataloader:\n","    batch = {k: v.to(device) for k, v in batch.items()}\n","    with torch.no_grad():\n","        outputs = model(**batch)\n","\n","    logits = outputs.logits\n","    predictions = torch.argmax(logits, dim=-1)\n","    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n","\n","metric.compute()"]},{"cell_type":"markdown","id":"fee13854-f618-4434-a426-a3dbb1d6df54","metadata":{"id":"fee13854-f618-4434-a426-a3dbb1d6df54"},"source":["# Fine-tuning a model with the Trainer API"]},{"cell_type":"code","execution_count":null,"id":"470bda8a-7d40-49db-b842-760dab551563","metadata":{"id":"470bda8a-7d40-49db-b842-760dab551563"},"outputs":[],"source":["from datasets import load_dataset\n","from transformers import AutoTokenizer, DataCollatorWithPadding\n","\n","raw_datasets = load_dataset(\"glue\", \"mrpc\")\n","checkpoint = \"bert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","\n","\n","def tokenize_function(example):\n","    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n","\n","\n","tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":null,"id":"7f5a1a0b-97a9-43c1-ab4e-11529e6630a0","metadata":{"id":"7f5a1a0b-97a9-43c1-ab4e-11529e6630a0"},"outputs":[],"source":["from transformers import TrainingArguments\n","\n","training_args = TrainingArguments(\"test-trainer\")"]},{"cell_type":"code","execution_count":null,"id":"d2a63e9e-3064-495b-bdea-cccb99b89f03","metadata":{"id":"d2a63e9e-3064-495b-bdea-cccb99b89f03","outputId":"c78a5f9e-364b-43ff-de36-828ae49d93a6"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from transformers import AutoModelForSequenceClassification\n","\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"]},{"cell_type":"code","execution_count":null,"id":"38e2084f-07d9-439b-913b-4ccf119dcbea","metadata":{"id":"38e2084f-07d9-439b-913b-4ccf119dcbea"},"outputs":[],"source":["from transformers import Trainer\n","\n","trainer = Trainer(\n","    model,\n","    training_args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n",")"]},{"cell_type":"code","execution_count":null,"id":"5f55895a-3792-4ad6-94fa-5bc0de326c70","metadata":{"id":"5f55895a-3792-4ad6-94fa-5bc0de326c70","outputId":"9c484ba3-1809-4b84-d0ad-40ebc13f92a7"},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1377/1377 13:05, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.501500</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.264500</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["TrainOutput(global_step=1377, training_loss=0.3116772221583946, metrics={'train_runtime': 791.2466, 'train_samples_per_second': 13.907, 'train_steps_per_second': 1.74, 'total_flos': 405114969714960.0, 'train_loss': 0.3116772221583946, 'epoch': 3.0})"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]},{"cell_type":"code","execution_count":null,"id":"4d183825-c9fb-46ef-a7df-57a1682e55df","metadata":{"id":"4d183825-c9fb-46ef-a7df-57a1682e55df","outputId":"2368fdc9-f6f5-4f6f-c1ed-3757dc6ab67b"},"outputs":[{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["(408, 2) (408,)\n"]}],"source":["predictions = trainer.predict(tokenized_datasets[\"validation\"])\n","print(predictions.predictions.shape, predictions.label_ids.shape)"]},{"cell_type":"code","execution_count":null,"id":"80901a4d-8495-4c9e-b1f4-c13a1ded3994","metadata":{"id":"80901a4d-8495-4c9e-b1f4-c13a1ded3994"},"outputs":[],"source":["import numpy as np\n","\n","preds = np.argmax(predictions.predictions, axis=-1)"]},{"cell_type":"code","execution_count":null,"id":"212d5638-7d48-44b5-93e7-a33af4cc0de5","metadata":{"id":"212d5638-7d48-44b5-93e7-a33af4cc0de5","outputId":"ce191244-1d25-4730-a6a4-23a6e6d2bf3f"},"outputs":[{"data":{"text/plain":["{'accuracy': 0.8627450980392157, 'f1': 0.903448275862069}"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["import evaluate\n","\n","metric = evaluate.load(\"glue\", \"mrpc\")\n","metric.compute(predictions=preds, references=predictions.label_ids)"]},{"cell_type":"code","execution_count":null,"id":"3778c5ca-361c-49d4-ada1-450bc88ef161","metadata":{"id":"3778c5ca-361c-49d4-ada1-450bc88ef161"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"mix","language":"python","name":"mix"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}